{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ab3d2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import re\n",
    "import itertools\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "929c9f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_links(elements):\n",
    "    image_links = []\n",
    "    for link in elements:\n",
    "        try:\n",
    "            link.attrs['src']\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            image_links.append(link.attrs['src'])\n",
    "    return image_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a36cff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(image_links, start_number, dir_path, word):\n",
    "    error_counter = 0\n",
    "    \n",
    "    for i, imageURL in enumerate(image_links):\n",
    "#         print(str(start_number+i+1))\n",
    "#         print(\"------------------------\")\n",
    "        #Try catching image, if error occurs, execute except program\n",
    "        try:\n",
    "            file_path = dir_path + \"/\" + str(start_number+i+1)+\".jpg\"\n",
    "            urllib.request.urlretrieve(imageURL, file_path)\n",
    "        except Exception as e:\n",
    "            error_counter += 1\n",
    "\n",
    "#         if i == 0:\n",
    "#             print(\"Start downloading\", word, \"images\")\n",
    "#         if i == len(image_links)-1:\n",
    "#             print(\"End downloading\", word, \"images\")\n",
    "#             print()\n",
    "#             print()\n",
    "        if i==200:\n",
    "            break\n",
    "            print(\"{} errors occur out of \".format(i, error_counter)) \n",
    "    start_number += len(image_links)\n",
    "\n",
    "    return start_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "120c4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_the_search_request(word, start):\n",
    "    urlKeyword = urllib.parse.quote(word)\n",
    "\n",
    "    url = 'https://www.google.com/search?q=' + urlKeyword + '&tbm=isch&start='+str(start)+'&num=94'#&btnG=Google+Search&tbs=0&safe=off&tbm=isch'\n",
    "    headers = {\"User-Agent\": \"Chrome/93.0.4577.82 Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\",}\n",
    "\n",
    "    request = urllib.request.Request(url=url, headers=headers)\n",
    "\n",
    "#     url = 'https://www.google.com/search?hl=jpg&q=' + urlKeyword + '&btnG=Google+Search&tbs=0&safe=off&tbm=isch'\n",
    "#     headers = {\"User-Agent\": \"Chrome/93.0.4577.82 Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\",}\n",
    "#     request = urllib.request.Request(url=url, headers=headers)\n",
    "    \n",
    "    return request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f829e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_search_results(request):\n",
    "    page = urllib.request.urlopen(request)\n",
    "    html = page.read().decode('utf-8')\n",
    "    html = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    elements = html.find_all('img')\n",
    "\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7f7b943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_image_extraction(search_categories, search_words, img_dir):\n",
    "    for category in search_categories:\n",
    "        dir_path = img_dir + category\n",
    "        if not os.path.exists(dir_path):\n",
    "                os.makedirs(dir_path)\n",
    "\n",
    "        start_number = 0 \n",
    "        for word in search_words[category]:\n",
    "            all_elements=[]\n",
    "            for i in range(1,4):\n",
    "                request = build_the_search_request(word, i)\n",
    "                elems = get_the_search_results(request)\n",
    "                all_elements += elems\n",
    "\n",
    "            image_links = extract_image_links(all_elements)  \n",
    "            start_number = download_images(image_links, start_number, dir_path, word)\n",
    "        \n",
    "        print(f\"Total number of images downloeded for {category} is {start_number}\")\n",
    "        print(\"********************************************************************\")\n",
    "        print()\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aa0504d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images downloeded for paper is 315\n",
      "********************************************************************\n",
      "\n",
      "Total number of images downloeded for glass is 756\n",
      "********************************************************************\n",
      "\n",
      "Total number of images downloeded for paper is 315\n",
      "********************************************************************\n",
      "\n",
      "Total number of images downloeded for cardboard is 504\n",
      "********************************************************************\n",
      "\n",
      "Total number of images downloeded for plastic bottle is 315\n",
      "********************************************************************\n",
      "\n",
      "Total number of images downloeded for tissue is 504\n",
      "********************************************************************\n",
      "\n",
      "Total number of images downloeded for food wrapper is 378\n",
      "********************************************************************\n",
      "\n",
      "Total number of images downloeded for plastic bags is 315\n",
      "********************************************************************\n",
      "\n",
      "Total number of images downloeded for pizza box is 378\n",
      "********************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_categories = [\"paper\",\"glass\", \"paper\", \"cardboard\", \"plastic bottle\", \"tissue\", \"food wrapper\", \"plastic bags\", \"pizza box\"]\n",
    "\n",
    "\n",
    "search_words={\"glass\": ['\"glass\" bottle', '\"glass\" bottles', \n",
    "                                '\"glass\" milk bottle', '\"glass\" milk bottles',\n",
    "                                '\"glass\" soda bottles', '\"glass\" soda bottle', \n",
    "                                '\"glass\" juice bottles', '\"glass\" juice bottle',\n",
    "                                '\"empty glass\" bottle', '\"empty glass\" bottles',\n",
    "                                '\"glass\" jar', '\"glass\" jam jar'],\n",
    "              \"paper\":['paper', '\"paper\" object', '\"paper\" crumpled', 'shredded paper', '\"paper\" bag'],\n",
    "              \"cardboard\":['cardboard', '\"cardboard\" box', 'amazon box', 'delivery \"cardboard\" box', 'cardboard milk cartons', 'cartons', 'juice cartons', 'tissue box'],\n",
    "              \"plastic bottle\" : ['plastic bottle', 'plastic water bottle', 'plastic milk bottle', 'plastic milk container', 'plastic juice bottles'],\n",
    "              \"tissue\" : ['tissue crumpled', 'paper napkin crumpled', 'used tissue', 'toilet paper', 'paper napkin', 'toilet paper', 'kitchen paper', 'kitchen paper towel'],\n",
    "              \"food wrapper\" : ['food wrapper', 'food paper wrapper', 'food paper', 'greasy food box', 'greasy food wrapper', 'greasy food paper'],\n",
    "              \"plastic bags\" : ['plastic bags', 'sainsburys plastic bags', 'morrisons plastic bags', 'tesco plastic bags', 'coop plastic bags'],\n",
    "              \"pizza box\" : ['pizza box', 'greasy pizza box', 'franca manca pizza box', 'pizza hut pizza box', 'dominos pizza box','papa johns pizza box']\n",
    "             }\n",
    "\n",
    "img_dir = \"./data/\"\n",
    "google_image_extraction(search_categories, search_words, img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d20afca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir('./data/paper')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06add69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for category in search_categories:\n",
    "#     dir_path = img_dir + category\n",
    "#     if not os.path.exists(dir_path):\n",
    "#             os.makedirs(dir_path)\n",
    "#     start_number = 0 \n",
    "#     for key, value in search_words.items():\n",
    "#         for word in value:\n",
    "#             urlKeyword = urllib.parse.quote(word)\n",
    "#             #Create url with target word\n",
    "#             url = 'https://www.google.com/search?hl=jp&q=' + urlKeyword + '&btnG=Google+Search&tbs=0&safe=off&tbm=isch'\n",
    "\n",
    "#             # headers is necessary when you send request\n",
    "#             headers = {\"User-Agent\": \"Chrome/83.0.4103.116 Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\",}\n",
    "           \n",
    "#             #Request is made with url and headers\n",
    "#             request = urllib.request.Request(url=url, headers=headers)\n",
    "#             page = urllib.request.urlopen(request)\n",
    "#             html = page.read().decode('utf-8')\n",
    "#             html = BeautifulSoup(html, \"html.parser\")\n",
    "#             elems = html.find_all('img')\n",
    "\n",
    "#             image_links = []\n",
    "#             for link in elems:\n",
    "#                 try:\n",
    "#                     link.attrs['data-src']\n",
    "#                 except:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     image_links.append(link.attrs['data-src'])\n",
    "\n",
    "#             for i, imageURL in enumerate(image_links):\n",
    "\n",
    "#                 #Try catching image, if error occurs, execute except program\n",
    "#                 try:\n",
    "#                     file_path = dir_path + \"/\" + str(start_number+i+1)+\".jpg\"\n",
    "#                     urllib.request.urlretrieve(imageURL, file_path)\n",
    "#                 except Exception as e:\n",
    "#                     error_counter += 1\n",
    "\n",
    "#                 if i == 0:\n",
    "#                     print(\"Start downloading\", word, \"images\")\n",
    "#                 if i == len(image_links)-1:\n",
    "#                     print(\"End downloading\", word, \"images\")\n",
    "#                 if i==200:\n",
    "#                     break\n",
    "#                     print(\"{} errors occur out of \".format(i, error_counter)) \n",
    "#             start_number += len(image_links)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fe680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
